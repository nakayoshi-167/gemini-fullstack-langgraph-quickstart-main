import os

from agent.tools_and_schemas import (
    SearchQueryList, 
    Reflection, 
    ResearchPlan, 
    StructuredResearchPlan, 
    SubTopicResearch, 
    CitedAnswer, 
    CritiqueAssessment,
    # Academic Research Framework Schemas
    AcademicBackground,
    AcademicFramework,
    AcademicAbstract,
    LiteratureResearch,
    AcademicReview
)
from dotenv import load_dotenv
from langchain_core.messages import AIMessage
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from google.genai import Client

from agent.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    ResearchPlanState,
    WebSearchState,
    PlannerState,
    ParallelResearchState,
    SynthesisState,
    CritiqueState,
    # Academic Research Framework State Classes
    AcademicBackgroundState,
    AcademicFrameworkState,
    AcademicAbstractState,
    LiteratureResearchState,
    AcademicReviewState,
)
from agent.configuration import Configuration
from agent.prompts import (
    get_current_date,
    research_plan_instructions,
    query_writer_instructions,
    web_searcher_instructions,
    PLANNER_PROMPT,
    RESEARCHER_PROMPT,
    SYNTHESIZER_PROMPT,
    CRITIQUE_PROMPT,
    # Academic Research Framework Prompts
    ACADEMIC_BACKGROUND_PROMPT,
    ACADEMIC_FRAMEWORK_PROMPT,
    ABSTRACT_GENERATOR_PROMPT,
    LITERATURE_RESEARCH_PROMPT,
    ACADEMIC_REPORT_SYNTHESIS_PROMPT,
    ACADEMIC_REVIEW_PROMPT
)
from langchain_google_genai import ChatGoogleGenerativeAI
from agent.utils import (
    get_citations,
    get_research_topic,
    insert_citation_markers,
    resolve_urls
)
from agent.history import history_manager
import time

load_dotenv()

if os.getenv("GEMINI_API_KEY") is None:
    raise ValueError("GEMINI_API_KEY is not set")

# Used for Google Search API
genai_client = Client(api_key=os.getenv("GEMINI_API_KEY"))


# Enhanced Multi-Agent Nodes for Deep Research Architecture

def enhanced_planner(state: OverallState, config: RunnableConfig) -> PlannerState:
    """Enhanced planner that creates structured research plan with sub-topics.
    
    Creates a hierarchical research plan with multiple sub-topics that can be executed 
    by parallel researcher agents. This approach enables comprehensive coverage of 
    complex topics through focused, parallel investigation of different aspects.
    """
    configurable = Configuration.from_runnable_config(config)
    
    # Initialize Gemini 2.5 Pro for enhanced planning
    llm = ChatGoogleGenerativeAI(
        model=configurable.query_generator_model,
        temperature=0.3,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(StructuredResearchPlan)
    
    # Format the enhanced planner prompt
    user_question = get_research_topic(state["messages"])
    formatted_prompt = PLANNER_PROMPT.format(user_question=user_question)
    
    # Generate the structured research plan
    result = structured_llm.invoke(formatted_prompt)
    
    # Convert to state format
    sub_topics_list = []
    for sub_topic in result.sub_topics:
        sub_topics_list.append({
            "topic_name": sub_topic.topic_name,
            "search_queries": sub_topic.search_queries
        })
    
    return {
        "structured_plan": {
            "research_question": result.research_question,
            "sub_topics": sub_topics_list,
            "estimated_depth": result.estimated_depth
        },
        "current_phase": "planning",
        "start_time": time.time(),
        "original_query": user_question,
        "effort_level": "comprehensive",  # Enhanced planning implies comprehensive research
        "revision_count": 0
    }


def run_parallel_research(state: OverallState, config: RunnableConfig):
    """Dispatcher node that launches parallel researcher agents.
    
    Distributes research sub-topics to multiple specialized researcher agents that can 
    work in parallel. Uses LangGraph's Send directive to spawn multiple parallel research branches,
    enabling efficient and comprehensive information gathering across different aspects of the topic.
    """
    structured_plan = state.get("structured_plan", {})
    sub_topics = structured_plan.get("sub_topics", [])
    
    # Create parallel Send directives for each sub-topic
    parallel_sends = []
    for idx, sub_topic in enumerate(sub_topics):
        parallel_sends.append(
            Send(
                "focused_researcher",
                {
                    "topic_name": sub_topic["topic_name"],
                    "search_queries": sub_topic["search_queries"],
                    "sub_topic_id": str(idx)
                }
            )
        )
    
    return parallel_sends


def focused_researcher(state: ParallelResearchState, config: RunnableConfig) -> OverallState:
    """Focused researcher agent for single sub-topic.
    
    Specialized researcher that focuses exclusively on one sub-topic to ensure deep, 
    comprehensive coverage without interference from other topics. This targeted approach
    enables higher quality research results for each specific area of investigation.
    """
    configurable = Configuration.from_runnable_config(config)
    
    # Format the specialized researcher prompt
    formatted_prompt = RESEARCHER_PROMPT.format(sub_topic=state["topic_name"])
    
    # Use Google Search for this sub-topic
    try:
        response = genai_client.models.generate_content(
            model=configurable.query_generator_model,
            contents=formatted_prompt,
            config={
                "tools": [{"google_search": {}}],
                "temperature": 0,
            },
        )
        
        # Safely process citations for this sub-topic
        grounding_chunks = None
        if (response and 
            hasattr(response, 'candidates') and 
            response.candidates and 
            len(response.candidates) > 0 and
            hasattr(response.candidates[0], 'grounding_metadata') and
            response.candidates[0].grounding_metadata and
            hasattr(response.candidates[0].grounding_metadata, 'grounding_chunks')):
            grounding_chunks = response.candidates[0].grounding_metadata.grounding_chunks
        
        if grounding_chunks is not None:
            resolved_urls = resolve_urls(grounding_chunks, state["sub_topic_id"])
            citations = get_citations(response, resolved_urls)
            modified_text = insert_citation_markers(response.text, citations) if citations else (response.text if response else "")
            sources_gathered = []
            if citations:
                for citation in citations:
                    if citation and "segments" in citation and citation["segments"]:
                        sources_gathered.extend(citation["segments"])
        else:
            # Fallback when no grounding metadata available
            modified_text = response.text if response and hasattr(response, 'text') else "è©²å½“ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            sources_gathered = []
            
    except Exception as e:
        print(f"ðŸš¨ Error in focused_researcher for topic '{state.get('topic_name', 'unknown')}': {e}")
        modified_text = "è©²å½“ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        sources_gathered = []
    
    # Format as structured sub-topic research
    research_result = f"""
## {state["topic_name"]}

{modified_text}

### æƒ…å ±æº
{chr(10).join([f"- [{source['label']}]({source['value']})" for source in sources_gathered[:5]])}
"""
    
    return {
        "parallel_research_results": [research_result],
        "sources_gathered": sources_gathered,
    }


def aggregate_research_results(state: OverallState, config: RunnableConfig):
    """Aggregation node that waits for all parallel research to complete.
    
    This node simply passes through the state after all parallel research is complete.
    It acts as a synchronization point before synthesis.
    """
    # Brief processing to ensure parallel results are properly aggregated
    parallel_results_count = len(state.get("parallel_research_results", []))
    sources_count = len(state.get("sources_gathered", []))
    
    print(f"ðŸ”„ Aggregating {parallel_results_count} research results with {sources_count} sources...")
    
    return {
        "current_phase": "researching"  # Single phase update after all parallel research
    }


def synthesizer(state: OverallState, config: RunnableConfig) -> SynthesisState:
    """Synthesizer agent that integrates all parallel research results.
    
    Integration specialist that combines findings from multiple parallel research agents 
    into a single coherent, well-structured report. Ensures logical flow and consistency 
    across different research areas while maintaining comprehensive coverage.
    """
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    
    # Initialize LLM for synthesis
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0.2,  # Lower temperature for more consistent synthesis
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    
    # Combine all research results
    research_results = "\n\n---\n\n".join(state.get("parallel_research_results", []))
    research_question = state.get("structured_plan", {}).get("research_question", get_research_topic(state["messages"]))
    
    # Format synthesizer prompt
    formatted_prompt = SYNTHESIZER_PROMPT.format(
        research_results=research_results,
        research_question=research_question
    )
    
    # Generate draft report
    result = llm.invoke(formatted_prompt)
    
    return {
        "draft_report": result.content,
        "current_phase": "synthesizing"
    }


def critique_agent(state: OverallState, config: RunnableConfig) -> CritiqueState:
    """Critique agent for quality assurance.
    
    Quality assurance specialist that evaluates research reports for accuracy, completeness,
    and overall quality. Provides detailed feedback and recommendations for improvement,
    enabling iterative refinement through a self-correction feedback loop.
    """
    MAX_REVISIONS = 1  # STRICT LIMIT: Match other functions
    current_revisions = state.get("revision_count", 0)
    
    print(f"ðŸ“ CRITIQUE START - Current revisions: {current_revisions}/{MAX_REVISIONS}")
    
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.reflection_model
    
    # Check if we've already reached the revision limit
    if current_revisions >= MAX_REVISIONS:
        print(f"ðŸš« CRITIQUE: Revision limit already reached ({current_revisions})")
        print(f"ðŸš« FORCING should_revise=False to prevent infinite loop")
        return {
            "critique_feedback": "å“è³ªè©•ä¾¡: ä¿®æ­£å›žæ•°ã®åˆ¶é™ã«é”ã—ãŸãŸã‚ã€ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã§å®Œäº†ã—ã¾ã™ã€‚",
            "should_revise": False,  # FORCE False to prevent infinite loop
            "revision_suggestions": [],
            "current_phase": "critiquing_completed"
        }
    
    # Initialize LLM for critique
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0.7,  # Higher temperature for more creative critique
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(CritiqueAssessment)
    
    # Format critique prompt
    draft_report = state.get("draft_report", "")
    formatted_prompt = CRITIQUE_PROMPT.format(draft_report=draft_report)
    
    # Generate critique
    result = structured_llm.invoke(formatted_prompt)
    
    # SAFETY CHECK: Force should_revise to False if we would exceed limit
    original_should_revise = result.should_revise
    if current_revisions >= MAX_REVISIONS:
        result.should_revise = False
        print(f"ðŸ›¡ï¸ SAFETY: Forced should_revise from {original_should_revise} to False")
    
    print(f"ðŸ“ CRITIQUE RESULT: should_revise={result.should_revise} (original: {original_should_revise})")
    print(f"ðŸ“ Current revision count: {current_revisions}")
    
    return {
        "critique_feedback": f"å“è³ªè©•ä¾¡: {result.overall_quality}\n\nå¼·ã¿: {', '.join(result.strengths)}\n\næ”¹å–„ç‚¹: {', '.join(result.weaknesses)}\n\nå…·ä½“çš„ææ¡ˆ: {', '.join(result.specific_suggestions)}",
        "should_revise": result.should_revise,
        "revision_suggestions": result.specific_suggestions,
        "current_phase": "critiquing"
    }


def evaluate_report_quality(state: CritiqueState, config: RunnableConfig):
    """Routing function that determines whether to revise or finalize the report.
    
    Controls the critique loop based on quality assessment with STRICT limits.
    """
    MAX_REVISIONS = 1  # STRICT LIMIT: Only allow 1 revision to prevent infinite loops
    current_revisions = state.get("revision_count", 0)
    should_revise = state.get("should_revise", False)
    
    print(f"ðŸ” QUALITY EVALUATION - Current revisions: {current_revisions}/{MAX_REVISIONS}")
    print(f"ðŸ” Should revise: {should_revise}")
    print(f"ðŸ” State revision_count: {state.get('revision_count', 'NOT_SET')}")
    
    # ABSOLUTE HARD LIMIT: Never exceed 1 revision under any circumstances
    if current_revisions >= MAX_REVISIONS:
        print(f"ðŸš« REVISION LIMIT REACHED: {current_revisions} >= {MAX_REVISIONS}")
        print(f"ðŸš« FORCING FINAL POLISH - No more revisions allowed")
        return "final_polish"
    
    # Only proceed with revision if we haven't hit the limit AND should_revise is True
    if should_revise and current_revisions < MAX_REVISIONS:
        print(f"âœ… PROCEEDING TO REVISION #{current_revisions + 1}/{MAX_REVISIONS}")
        return "revise_report"
    else:
        print(f"âœ… PROCEEDING TO FINAL POLISH (revisions: {current_revisions}, should_revise: {should_revise})")
        return "final_polish"


def revise_report(state: OverallState, config: RunnableConfig) -> SynthesisState:
    """Revises the report based on critique feedback.
    
    Implements SINGLE revision with strict limits to prevent infinite loops.
    """
    MAX_REVISIONS = 1  # STRICT LIMIT: Match the evaluation function
    current_revisions = state.get("revision_count", 0)
    
    print(f"ðŸ”„ REVISION START - Current: {current_revisions}/{MAX_REVISIONS}")
    print(f"ðŸ”„ State before revision: revision_count = {state.get('revision_count', 'NOT_SET')}")
    
    # CRITICAL SAFETY CHECK: Never allow more than 1 revision
    if current_revisions >= MAX_REVISIONS:
        print(f"ðŸš¨ EMERGENCY STOP - Maximum revisions ({current_revisions}) reached")
        print(f"ðŸš¨ Aborting revision, returning current draft")
        return {
            "draft_report": state.get("draft_report", ""),
            "revision_count": current_revisions,  # Don't increment
            "current_phase": "emergency_stopped"
        }
    
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    
    # Initialize LLM for revision
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0.3,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    
    # Create revision prompt incorporating feedback
    revision_prompt = f"""
ã‚ãªãŸã¯å°‚é–€ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‰ãƒ©ãƒ•ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ã€æä¾›ã•ã‚ŒãŸæ‰¹è©•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ã„ã¦æ”¹å–„ã—ã¦ãã ã•ã„ã€‚

## æ‰¹è©•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
{state.get("critique_feedback", "")}

## æ”¹å–„ã™ã¹ãç‚¹
{chr(10).join(f"- {suggestion}" for suggestion in state.get("revision_suggestions", []))}

## ç¾åœ¨ã®ãƒ‰ãƒ©ãƒ•ãƒˆ
{state.get("draft_report", "")}

## æŒ‡ç¤º
- æ‰¹è©•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ³¨æ„æ·±ãæ¤œè¨Žã—ã€æŒ‡æ‘˜ã•ã‚ŒãŸå•é¡Œã‚’ä¿®æ­£ã—ã¦ãã ã•ã„
- æ—¥æœ¬èªžã§è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„
- ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚’ä¿æŒã—ã¦ãã ã•ã„
- å¼•ç”¨ã‚’é©åˆ‡ã«ç¶­æŒã—ã¦ãã ã•ã„
- æ§‹é€ ã¨è«–ç†çš„ãªæµã‚Œã‚’æ”¹å–„ã—ã¦ãã ã•ã„

æ”¹å–„ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
"""
    
    # Generate revised report
    result = llm.invoke(revision_prompt)
    
    new_revision_count = current_revisions + 1
    print(f"âœ… REVISION COMPLETED - New count: {new_revision_count}/{MAX_REVISIONS}")
    print(f"âœ… This was the FINAL allowed revision")
    
    return {
        "draft_report": result.content,
        "revision_count": new_revision_count,
        "current_phase": "revising"
    }


def final_polish(state: OverallState, config: RunnableConfig):
    """Final polishing and completion of the research report.
    
    Applies final touches and saves the completed report.
    """
    # Use the latest draft as the final report
    final_content = state.get("draft_report", "")
    
    # Add completion footer with research summary
    parallel_results_count = len(state.get("parallel_research_results", []))
    sources_count = len(state.get("sources_gathered", []))
    revision_count = state.get("revision_count", 0)
    
    metadata_footer = f"""

---

## ðŸ”¬ ç ”ç©¶åˆ†æžãƒ¬ãƒãƒ¼ãƒˆå®Œäº†

**èª¿æŸ»ã‚µãƒžãƒªãƒ¼:**
- **ä¸¦åˆ—ãƒªã‚µãƒ¼ãƒã‚»ãƒƒã‚·ãƒ§ãƒ³**: {parallel_results_count}ä»¶
- **åŽé›†æƒ…å ±æº**: {sources_count}ä»¶  
- **å“è³ªæ”¹å–„ãƒ©ã‚¦ãƒ³ãƒ‰**: {revision_count}å›žï¼ˆä¸Šé™1å›žï¼‰

**ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ **: é«˜åº¦ãªãƒžãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒ‡ã‚£ãƒ¼ãƒ—ãƒªã‚µãƒ¼ãƒãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯  
**ç”Ÿæˆæ—¥æ™‚**: {get_current_date()}

---

*æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯ã€å°‚é–€åŒ–ã•ã‚ŒãŸAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã€ä¸¦åˆ—ãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã€çµ±åˆåˆ†æžå®˜ã€å“è³ªè©•ä¾¡å®˜ï¼‰ã«ã‚ˆã£ã¦æ®µéšŽçš„ã«ä½œæˆã•ã‚Œã€åŽ³æ ¼ãªå“è³ªç®¡ç†ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆæœ€å¤§1å›žã®ä¿®æ­£ï¼‰ã‚’çµŒã¦å®Œæˆã•ã‚Œã¾ã—ãŸã€‚*
"""
    
    final_content_with_metadata = final_content + metadata_footer
    
    # Save to history (existing functionality)
    try:
        duration_ms = int((time.time() - state.get("start_time", time.time())) * 1000)
        search_queries = [result for result in state.get("parallel_research_results", [])]
        sources_count = len(state.get("sources_gathered", []))
        
        history_id = history_manager.save_history(
            query=state.get("original_query", get_research_topic(state["messages"])),
            effort=state.get("effort_level", "comprehensive"),
            model=state.get("reasoning_model", "gemini-2.5-pro"),
            result=final_content_with_metadata,
            search_queries=search_queries[:10],  # Limit to avoid excessive storage
            sources_count=sources_count,
            duration_ms=duration_ms
        )
        
        print(f"Enhanced research completed and saved: {history_id}")
        
    except Exception as e:
        print(f"History save error in enhanced research: {e}")

    return {
        "messages": [AIMessage(content=final_content_with_metadata)],
        "final_report": final_content_with_metadata,
        "current_phase": "completed"
    }


# Original nodes (preserved for backward compatibility)
def create_research_plan(state: OverallState, config: RunnableConfig) -> ResearchPlanState:
    """LangGraph node that creates a structured research plan based on the user's question.
    
    This implements Step 1 of the DeepResearch algorithm: Query Input and Research Plan Creation.
    Creates a strategic outline for the research report that will guide subsequent information gathering.
    
    Args:
        state: Current graph state containing the user's question
        config: Configuration for the runnable, including LLM provider settings
    
    Returns:
        Dictionary with state update, including research_plan containing sections and rationale
    """
    configurable = Configuration.from_runnable_config(config)
    
    # Initialize Gemini 2.5 Pro for plan creation
    llm = ChatGoogleGenerativeAI(
        model=configurable.query_generator_model,
        temperature=0.3,  # Lower temperature for more structured planning
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(ResearchPlan)
    
    # Format the prompt for research plan creation
    current_date = get_current_date()
    formatted_prompt = research_plan_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
    )
    
    # Generate the research plan
    result = structured_llm.invoke(formatted_prompt)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
    original_query = get_research_topic(state["messages"])
    effort_level = "medium"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã€å®Ÿéš›ã®å€¤ã¯ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰æ¸¡ã•ã‚Œã‚‹
    
    return {
        "research_plan": {
            "sections": result.sections,
            "rationale": result.rationale
        },
        "plan_approved": True,
        "start_time": time.time(),
        "original_query": original_query,
        "effort_level": effort_level
    }


def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """LangGraph node that generates search queries based on the User's question.

    Uses Gemini 2.5 Pro to create an optimized search queries for web research based on
    the User's question.

    Args:
        state: Current graph state containing the User's question
        config: Configuration for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including search_query key containing the generated queries
    """
    configurable = Configuration.from_runnable_config(config)

    # check for custom initial search query count
    if state.get("initial_search_query_count") is None:
        state["initial_search_query_count"] = configurable.number_of_initial_queries

    # init Gemini 2.5 Pro
    llm = ChatGoogleGenerativeAI(
        model=configurable.query_generator_model,
        temperature=1.0,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(SearchQueryList)

    # Format the prompt - now uses Japanese instructions by default
    current_date = get_current_date()
    formatted_prompt = query_writer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        number_queries=state["initial_search_query_count"],
    )
    
    # Generate the search queries
    result = structured_llm.invoke(formatted_prompt)
    return {"search_query": result.query}


def continue_to_web_research(state: QueryGenerationState):
    """LangGraph node that sends the search queries to the web research node.

    This is used to spawn n number of web research nodes, one for each search query.
    """
    return [
        Send("web_research", {"search_query": search_query, "id": int(idx)})
        for idx, search_query in enumerate(state["search_query"])
    ]


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """LangGraph node that performs web research using the native Google Search API tool.

    Executes a web search using the native Google Search API tool in combination with Gemini 2.5 Pro.

    Args:
        state: Current graph state containing the search query and research loop count
        config: Configuration for the runnable, including search API settings

    Returns:
        Dictionary with state update, including sources_gathered, research_loop_count, and web_research_results
    """
    # Configure
    configurable = Configuration.from_runnable_config(config)
    formatted_prompt = web_searcher_instructions.format(
        current_date=get_current_date(),
        research_topic=state["search_query"],
    )

    # Uses the google genai client as the langchain client doesn't return grounding metadata
    try:
        response = genai_client.models.generate_content(
            model=configurable.query_generator_model,
            contents=formatted_prompt,
            config={
                "tools": [{"google_search": {}}],
                "temperature": 0,
            },
        )
        
        # Safely process citations
        grounding_chunks = None
        if (response and 
            hasattr(response, 'candidates') and 
            response.candidates and 
            len(response.candidates) > 0 and
            hasattr(response.candidates[0], 'grounding_metadata') and
            response.candidates[0].grounding_metadata and
            hasattr(response.candidates[0].grounding_metadata, 'grounding_chunks')):
            grounding_chunks = response.candidates[0].grounding_metadata.grounding_chunks
        
        if grounding_chunks is not None:
            resolved_urls = resolve_urls(grounding_chunks, state["id"])
            citations = get_citations(response, resolved_urls)
            modified_text = insert_citation_markers(response.text, citations) if citations else (response.text if response else "")
            sources_gathered = []
            if citations:
                for citation in citations:
                    if citation and "segments" in citation and citation["segments"]:
                        sources_gathered.extend(citation["segments"])
        else:
            # Fallback when no grounding metadata available
            modified_text = response.text if response and hasattr(response, 'text') else "è©²å½“ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            sources_gathered = []
            
    except Exception as e:
        print(f"ðŸš¨ Error in web_research for query '{state.get('search_query', 'unknown')}': {e}")
        modified_text = "è©²å½“ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        sources_gathered = []

    return {
        "sources_gathered": sources_gathered,
        "search_query": [state["search_query"]],
        "web_research_result": [modified_text],
    }


def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    """LangGraph node that identifies knowledge gaps and generates potential follow-up queries.

    Analyzes the current summary to identify areas for further research and generates
    potential follow-up queries. Uses structured output to extract
    the follow-up query in JSON format.

    Args:
        state: Current graph state containing the running summary and research topic
        config: Configuration for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including search_query key containing the generated follow-up query
    """
    configurable = Configuration.from_runnable_config(config)
    # Increment the research loop count and get the reasoning model
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    reasoning_model = state.get("reasoning_model", configurable.reflection_model)

    # Deep Research approach - structured reflection with comprehensive analysis
    reflection_instructions = """
ã‚ãªãŸã¯é«˜åº¦ãªèª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚åŽé›†ã—ãŸã‚¦ã‚§ãƒ–èª¿æŸ»ã®è¦ç´„ã‚’è©•ä¾¡ã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæ·±å±¤èª¿æŸ»æ‰‹æ³•ã«å¾“ã£ã¦çŸ¥è­˜ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰¹å®šã—ã€æˆ¦ç•¥çš„ãªè¿½åŠ è³ªå•ã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã§ã™ã€‚

## æ®µéšŽçš„ãªåˆ†æžãƒ—ãƒ­ã‚»ã‚¹

1. **èª¿æŸ»ã®æ–¹å‘æ€§ã®ç¢ºèª**: ç¾åœ¨ã®æ—¥ä»˜ï¼ˆ{current_date}ï¼‰ã‚’è€ƒæ…®ã—ã€ãƒˆãƒ”ãƒƒã‚¯ã€Œ{research_topic}ã€ã®æ€§è³ªã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ï¼š
   - éŽåŽ»ã®å‡ºæ¥äº‹ï¼šç¢ºå®šã—ãŸäº‹å®Ÿã¨çµæžœã‚’ä¸­å¿ƒã«åˆ†æž
   - ç¾åœ¨é€²è¡Œä¸­ï¼šæœ€æ–°ã®å‹•å‘ã¨ç¾çŠ¶ã‚’é‡è¦–
   - æœªæ¥ã®äºˆæ¸¬ï¼šç™ºè¡¨æ¸ˆã¿æƒ…å ±ã¨å°‚é–€å®¶ã®åˆ†æžã‚’é‡è¦–
   - è¤‡åˆçš„ãƒˆãƒ”ãƒƒã‚¯ï¼šè¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰å¤šé¢çš„ã«åˆ†æž

2. **å¤šè§’çš„ãªæƒ…å ±æ•´ç†**: ãƒˆãƒ”ãƒƒã‚¯ã®æ€§è³ªã«å¿œã˜ã¦ã€ä»¥ä¸‹ã®è¦ç´ ã§æƒ…å ±ã‚’æ•´ç†ã—ã¦ãã ã•ã„ï¼š
   - **åŸºæœ¬æƒ…å ±ã¨èƒŒæ™¯**: æ¦‚è¦ã€æ­´å²çš„çµŒç·¯ã€é‡è¦æ€§
   - **ç¾çŠ¶åˆ†æž**: æœ€æ–°ã®çŠ¶æ³ã€ãƒ‡ãƒ¼ã‚¿ã€äº‹å®Ÿé–¢ä¿‚
   - **å¤šè§’çš„è¦–ç‚¹**: ç•°ãªã‚‹ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã®è¦–ç‚¹ã€æ§˜ã€…ãªå´é¢
   - **å½±éŸ¿ã¨æ„ç¾©**: ç¤¾ä¼šçš„ã€çµŒæ¸ˆçš„ã€æŠ€è¡“çš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ
   - **å°†æ¥å±•æœ›**: äºˆæ¸¬å¯èƒ½ãªç™ºå±•ã€èª²é¡Œã€æ©Ÿä¼š

3. **æ®µéšŽçš„ãªå†…å®¹è©•ä¾¡**: ä»¥ä¸‹ã®æ§‹é€ ã§ç¾åœ¨ã®èª¿æŸ»å†…å®¹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š
   - åŸºæœ¬çš„ãªäº‹å®Ÿé–¢ä¿‚ã¯ç¶²ç¾…ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
   - ç¾çŠ¶ã®å‹•å‘ã¨æœ€æ–°æƒ…å ±ã¯ååˆ†ã‹ï¼Ÿ
   - è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰ã®åˆ†æžãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
   - ç¤¾ä¼šçš„ãƒ»çµŒæ¸ˆçš„å½±éŸ¿ã®åˆ†æžã¯é©åˆ‡ã‹ï¼Ÿ
   - å°†æ¥ã®å±•æœ›ã¨èª²é¡Œã¯æ˜Žç¢ºåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

4. **ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã®è©•ä¾¡**: ä»¥ä¸‹ã®ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æºã‹ã‚‰ã®æƒ…å ±ãŒååˆ†å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ï¼š
   - é–¢é€£ã™ã‚‹å…¬å¼ã‚µã‚¤ãƒˆãƒ»çµ„ç¹”ï¼ˆæ”¿åºœæ©Ÿé–¢ã€ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã€å­¦è¡“æ©Ÿé–¢ï¼‰
   - ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆNHKã€æœæ—¥æ–°èžã€æ—¥çµŒæ–°èžã€Reutersã€BBCç­‰ï¼‰
   - æ¥­ç•Œå°‚é–€æ©Ÿé–¢ãƒ»ã‚µã‚¤ãƒˆï¼ˆæ¥­ç•Œå›£ä½“ã€å°‚é–€èªŒã€ã‚·ãƒ³ã‚¯ã‚¿ãƒ³ã‚¯ï¼‰

5. **çŸ¥è­˜ã‚®ãƒ£ãƒƒãƒ—ã®ç‰¹å®š**: ä¸Šè¨˜ã®åˆ†æžã«åŸºã¥ãã€ã¾ã ä¸è¶³ã—ã¦ã„ã‚‹é‡è¦ãªæƒ…å ±ã¯ä½•ã§ã™ã‹ï¼Ÿãƒˆãƒ”ãƒƒã‚¯ã®æœ¬è³ªã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€ã©ã®å´é¢ã‚’ã‚ˆã‚Šæ·±ãèª¿æŸ»ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ

6. **æˆ¦ç•¥çš„è¿½åŠ ã‚¯ã‚¨ãƒªã®ä½œæˆ**: ã“ã‚Œã‚‰ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹ãŸã‚ã«ã€çš„ç¢ºã§åˆ†æžçš„ãªè¿½åŠ ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆã‚’æ—¥æœ¬èªžã§ä½œæˆã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã‚‰ã®ã‚¯ã‚¨ãƒªã¯ã€å˜ãªã‚‹äº‹å®Ÿæƒ…å ±ã§ã¯ãªãã€æ·±ã„æ´žå¯Ÿã€å¤šè§’çš„ãªåˆ†æžã€é•·æœŸçš„ãªå½±éŸ¿ã‚’æ˜Žã‚‰ã‹ã«ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ãã ã•ã„ã€‚

ç¾åœ¨ã®æ—¥ä»˜: {current_date}

èª¿æŸ»è¦ç´„:
{summaries}
"""

    # Format the prompt
    current_date = get_current_date()
    formatted_prompt = reflection_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n\n---\n\n".join(state["web_research_result"]),
    )
    # init Reasoning Model
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=1.0,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    result = llm.with_structured_output(Reflection).invoke(formatted_prompt)

    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "follow_up_queries": result.follow_up_queries,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
    }


def evaluate_research(
    state: ReflectionState,
    config: RunnableConfig,
) -> OverallState:
    """LangGraph routing function that determines the next step in the research flow.

    Controls the research loop by deciding whether to continue gathering information
    or to finalize the summary based on the configured maximum number of research loops.

    Args:
        state: Current graph state containing the research loop count
        config: Configuration for the runnable, including max_research_loops setting

    Returns:
        String literal indicating the next node to visit ("web_research" or "finalize_summary")
    """
    configurable = Configuration.from_runnable_config(config)
    max_research_loops = (
        state.get("max_research_loops")
        if state.get("max_research_loops") is not None
        else configurable.max_research_loops
    )
    if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
        return "finalize_answer"
    else:
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state["number_of_ran_queries"] + int(idx),
                },
            )
            for idx, follow_up_query in enumerate(state["follow_up_queries"])
        ]


def finalize_answer(state: OverallState, config: RunnableConfig):
    """LangGraph node that finalizes the research summary.

    Prepares the final output by deduplicating and formatting sources, then
    combining them with the running summary to create a well-structured
    research report with proper citations.

    Args:
        state: Current graph state containing the running summary and sources gathered

    Returns:
        Dictionary with state update, including running_summary key containing the formatted final summary with sources
    """
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    
    # Deep Research approach - comprehensive structured analysis report
    answer_instructions = """
ã‚ãªãŸã¯é«˜åº¦ãªèª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚èª¿æŸ»ãƒˆãƒ”ãƒƒã‚¯ã€Œ{research_topic}ã€ã«ã¤ã„ã¦ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæ·±å±¤èª¿æŸ»æ‰‹æ³•ã«åŸºã¥ã„ã¦ã€æ·±ãåŒ…æ‹¬çš„ãªåˆ†æžãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**èª¿æŸ»ã®æ–¹å‘æ€§ã®æ˜Žç¤º**:
ã¾ãšã€ç¾åœ¨ã®æ—¥ä»˜ï¼ˆ{current_date}ï¼‰ã‚’è€ƒæ…®ã—ã¦èª¿æŸ»ã®æ–¹å‘æ€§ã‚’æ˜Žç¢ºã«ã—ã¦ãã ã•ã„ï¼š
- éŽåŽ»ã®å‡ºæ¥äº‹ï¼šç¢ºå®šã—ãŸäº‹å®Ÿã¨çµæžœã‚’ä¸­å¿ƒã¨ã—ãŸåˆ†æž
- ç¾åœ¨é€²è¡Œä¸­ï¼šæœ€æ–°ã®å‹•å‘ã¨ç¾çŠ¶ã‚’é‡è¦–ã—ãŸåˆ†æž
- æœªæ¥ã®äºˆæ¸¬ï¼šç™ºè¡¨æ¸ˆã¿æƒ…å ±ã¨å°‚é–€å®¶ã®åˆ†æžã«åŸºã¥ãåˆ†æž
- è¤‡åˆçš„ãƒˆãƒ”ãƒƒã‚¯ï¼šè¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰å¤šé¢çš„ãªåˆ†æž

**æ®µéšŽçš„ãªæƒ…å ±æ•´ç†ã¨ãƒ¬ãƒãƒ¼ãƒˆæ§‹æˆ**:
ä»¥ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆã«å¾“ã£ã¦ã€æ®µéšŽçš„ã§åŒ…æ‹¬çš„ãªåˆ†æžãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
{research_plan_sections}

ãƒˆãƒ”ãƒƒã‚¯ã®æ€§è³ªã«å¿œã˜ã¦ã€ä»¥ä¸‹ã®è¦ç´ ã‚’é©åˆ‡ã«çµ„ã¿åˆã‚ã›ã¦æ§‹æˆã—ã¦ãã ã•ã„ï¼š
1. **åŸºæœ¬æƒ…å ±ã¨èƒŒæ™¯** - ãƒˆãƒ”ãƒƒã‚¯ã®æ¦‚è¦ã€æ­´å²çš„çµŒç·¯ã€é‡è¦æ€§
2. **ç¾çŠ¶åˆ†æž** - æœ€æ–°ã®çŠ¶æ³ã€ãƒ‡ãƒ¼ã‚¿ã€äº‹å®Ÿé–¢ä¿‚
3. **å¤šè§’çš„è¦–ç‚¹** - ç•°ãªã‚‹ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã®è¦–ç‚¹ã€æ§˜ã€…ãªå´é¢ã‹ã‚‰ã®åˆ†æž
4. **å½±éŸ¿ã¨æ„ç¾©** - ç¤¾ä¼šçš„ã€çµŒæ¸ˆçš„ã€æŠ€è¡“çš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ
5. **å°†æ¥ã®å±•æœ›** - äºˆæ¸¬å¯èƒ½ãªç™ºå±•ã€èª²é¡Œã€æ©Ÿä¼š

**ãƒ¬ãƒãƒ¼ãƒˆä½œæˆæŒ‡ç¤º**:
1. **ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼**: å®Œå…¨ãªãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚é©åˆ‡ãªè¦‹å‡ºã—éšŽå±¤ï¼ˆ#, ##, ###ï¼‰ã€ãƒªã‚¹ãƒˆã€å¼·èª¿ï¼ˆ**bold**ï¼‰ã€å¼•ç”¨ã‚’æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚

2. **æ®µéšŽçš„ãªæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®åæ˜ **: æ§‹é€ åŒ–ã•ã‚ŒãŸæ·±å±¤èª¿æŸ»æ‰‹æ³•ã«å¾“ã„ã€èª¿æŸ»ã®éŽç¨‹ã¨ç™ºè¦‹ã‚’æ®µéšŽçš„ã«æ•´ç†ã—ã¦ãã ã•ã„ï¼š
   - èª¿æŸ»çµæžœã®æ¦‚è¦
   - ä¸»è¦ãªç™ºè¦‹äº‹é …ã®æ•´ç†
   - è©³ç´°åˆ†æžï¼ˆæ·±ã„æ´žå¯Ÿã€å¤šè§’çš„ãªè¦–ç‚¹ï¼‰
   - å°†æ¥å±•æœ›ã¨ç¤ºå”†

3. **ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã®é‡è¦–**: ä»¥ä¸‹ã®ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æºã‚’é‡è¦–ã—ã€æƒ…å ±ã®ä¿¡é ¼æ€§ã‚’ç¤ºã—ã¦ãã ã•ã„ï¼š
   - é–¢é€£ã™ã‚‹å…¬å¼ã‚µã‚¤ãƒˆãƒ»çµ„ç¹”ï¼ˆæ”¿åºœæ©Ÿé–¢ã€ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã€å­¦è¡“æ©Ÿé–¢ï¼‰
   - ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆNHKã€æœæ—¥æ–°èžã€æ—¥çµŒæ–°èžã€Reutersã€BBCç­‰ï¼‰
   - æ¥­ç•Œå°‚é–€æ©Ÿé–¢ãƒ»ã‚µã‚¤ãƒˆï¼ˆæ¥­ç•Œå›£ä½“ã€å°‚é–€èªŒã€ã‚·ãƒ³ã‚¯ã‚¿ãƒ³ã‚¯ï¼‰

4. **å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿ã¨äº‹å®Ÿ**: å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ãªæ—¥ä»˜ã€æ•°å€¤ã€åç§°ã€ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã¦ãã ã•ã„ã€‚

5. **æ·±ã„æ´žå¯Ÿã®æä¾›**: å˜ãªã‚‹äº‹å®Ÿã®ç¾…åˆ—ã§ã¯ãªãã€ä»¥ä¸‹ã‚’å«ã‚€æ·±ã„åˆ†æžã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
   - **èƒŒæ™¯ã¨æ–‡è„ˆã®åˆ†æž**: ãªãœã“ã®çŠ¶æ³ãŒç”Ÿã˜ã¦ã„ã‚‹ã®ã‹ï¼Ÿ
   - **å¤šè§’çš„è¦–ç‚¹**: ç•°ãªã‚‹ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã‹ã‚‰ã®è¦–ç‚¹
   - **å½±éŸ¿ã¨æ„ç¾©**: ç¤¾ä¼šã€çµŒæ¸ˆã€æŠ€è¡“ã¸ã®é•·æœŸçš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ
   - **èª²é¡Œã¨æ©Ÿä¼š**: ä»Šå¾Œã®èª²é¡Œã¨ç™ºå±•ã®å¯èƒ½æ€§

6. **æƒ…å ±æºã®æ´»ç”¨**: 
   - æä¾›ã•ã‚ŒãŸå¼•ç”¨ãƒžãƒ¼ã‚«ãƒ¼ï¼ˆ[1], [2]ãªã©ï¼‰ã‚’é©åˆ‡ã«ä½¿ç”¨
   - è¤‡æ•°ã®æƒ…å ±æºã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæ´žå¯Ÿã‚’çµ±åˆ
   - æƒ…å ±ã®æ—¥ä»˜ã‚’ç¢ºèªã—ã€ã„ã¤ã®æƒ…å ±ã‹ã‚’æ˜Žè¨˜

7. **æ‰¹åˆ¤çš„æ€è€ƒ**: 
   - æƒ…å ±ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡
   - è¤‡æ•°ã®æƒ…å ±æºã‚’æ¯”è¼ƒ
   - ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè¦‹è§£ã‚’æä¾›

8. **æ—¥æœ¬èªžã§ã®å‡ºåŠ›**: å…¨ã¦ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã¯è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªžã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

**ç¾åœ¨ã®æ—¥ä»˜**: {current_date}

**åˆ†æžå¯¾è±¡ã®èª¿æŸ»çµæžœ**:
{summaries}

**é‡è¦**: ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæ·±å±¤èª¿æŸ»æ‰‹æ³•ã«åŸºã¥ãæ®µéšŽçš„ã§ä½“ç³»çš„ãªæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’åæ˜ ã—ã€åŒ…æ‹¬çš„ã§ã‚ã‚ŠãªãŒã‚‰æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†æžãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ä½œæˆã—ã¦ãã ã•ã„ã€‚å¸¸ã«æ‰¹åˆ¤çš„æ€è€ƒã‚’ç”¨ã„ã€è¤‡æ•°ã®æƒ…å ±æºã‚’æ¯”è¼ƒã—ã€ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè¦‹è§£ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""

    # Format the prompt with research plan sections
    current_date = get_current_date()
    research_plan_sections = ""
    if state.get("research_plan") and "sections" in state["research_plan"]:
        research_plan_sections = "\n".join([f"- {section}" for section in state["research_plan"]["sections"]])
    else:
        # Fallback if no research plan exists
        research_plan_sections = "- ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒžãƒªãƒ¼\n- ä¸»è¦ç™ºè¡¨å†…å®¹ã®åˆ†æž\n- æˆ¦ç•¥çš„æ„å‘³ã¨ç«¶åˆã¸ã®å½±éŸ¿\n- å°†æ¥å±•æœ›ã¨ç¤ºå”†"
    
    formatted_prompt = answer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        research_plan_sections=research_plan_sections,
        summaries="\n---\n\n".join(state["web_research_result"]),
    )

    # init Reasoning Model, default to Gemini 2.5 Pro
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    result = llm.invoke(formatted_prompt)

    # Replace the short urls with the original urls and add all used urls to the sources_gathered
    unique_sources = []
    for source in state["sources_gathered"]:
        if source["short_url"] in result.content:
            result.content = result.content.replace(
                source["short_url"], source["value"]
            )
            unique_sources.append(source)

    # æ¤œç´¢å±¥æ­´ã‚’ä¿å­˜
    try:
        duration_ms = int((time.time() - state.get("start_time", time.time())) * 1000) if state.get("start_time") else None
        search_queries = state.get("search_query", [])
        sources_count = len(unique_sources)
        
        history_id = history_manager.save_history(
            query=state.get("original_query", get_research_topic(state["messages"])),
            effort=state.get("effort_level", "medium"),
            model=reasoning_model,
            result=result.content,
            search_queries=search_queries,
            sources_count=sources_count,
            duration_ms=duration_ms
        )
        
        print(f"æ¤œç´¢å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {history_id}")
        
    except Exception as e:
        print(f"æ¤œç´¢å±¥æ­´ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return {
        "messages": [AIMessage(content=result.content)],
        "sources_gathered": unique_sources,
    }


# Enhanced Multi-Agent Deep Research Graph (Primary Implementation)
enhanced_builder = StateGraph(OverallState, config_schema=Configuration)

# Add all enhanced multi-agent nodes
enhanced_builder.add_node("enhanced_planner", enhanced_planner)
enhanced_builder.add_node("focused_researcher", focused_researcher)
enhanced_builder.add_node("aggregate_research_results", aggregate_research_results)
enhanced_builder.add_node("synthesizer", synthesizer)
enhanced_builder.add_node("critique_agent", critique_agent)
enhanced_builder.add_node("revise_report", revise_report)
enhanced_builder.add_node("final_polish", final_polish)

# Build the enhanced multi-agent workflow using hierarchical planning and parallel execution:
# START -> Enhanced Planner -> Parallel Research -> Synthesis -> Critique -> (Revise or Polish) -> END

# Entry point: Enhanced hierarchical planning
enhanced_builder.add_edge(START, "enhanced_planner")

# Parallel research branches (dispatched directly from planner)
enhanced_builder.add_conditional_edges(
    "enhanced_planner", 
    run_parallel_research,
    ["focused_researcher"]
)

# All parallel research results flow to aggregator
enhanced_builder.add_edge("focused_researcher", "aggregate_research_results")

# Aggregator synchronizes and then flows to synthesizer
enhanced_builder.add_edge("aggregate_research_results", "synthesizer")

# Synthesizer creates draft, then goes to critique
enhanced_builder.add_edge("synthesizer", "critique_agent")

# Critique decides: revise or finalize
enhanced_builder.add_conditional_edges(
    "critique_agent", 
    evaluate_report_quality, 
    ["revise_report", "final_polish"]
)

# Revision loop back to critique
enhanced_builder.add_edge("revise_report", "critique_agent")

# Final polish leads to END
enhanced_builder.add_edge("final_polish", END)

# Compile enhanced graph
enhanced_graph = enhanced_builder.compile(name="enhanced-deepresearch-agent")


# Original Simple Graph (Preserved for backward compatibility)
simple_builder = StateGraph(OverallState, config_schema=Configuration)

# Define the nodes implementing the 5-step DeepResearch algorithm
simple_builder.add_node("create_research_plan", create_research_plan)  # Step 1: Research Plan Creation
simple_builder.add_node("generate_query", generate_query)              # Step 2: Initial Query Generation  
simple_builder.add_node("web_research", web_research)                  # Step 2: Web Research
simple_builder.add_node("reflection", reflection)                      # Step 3: Reflection & Knowledge Gap Analysis
simple_builder.add_node("finalize_answer", finalize_answer)            # Step 4: Final Documentation

# Build the simple workflow
simple_builder.add_edge(START, "create_research_plan")
simple_builder.add_edge("create_research_plan", "generate_query")
simple_builder.add_conditional_edges(
    "generate_query", continue_to_web_research, ["web_research"]
)
simple_builder.add_edge("web_research", "reflection")
simple_builder.add_conditional_edges(
    "reflection", evaluate_research, ["web_research", "finalize_answer"]
)
simple_builder.add_edge("finalize_answer", END)

simple_graph = simple_builder.compile(name="simple-deepresearch-agent")

# ============================================================================
# ACADEMIC RESEARCH FRAMEWORK AGENTS (å­¦è¡“è«–æ–‡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)
# ============================================================================

def academic_background_generator(state: OverallState, config: RunnableConfig) -> AcademicBackgroundState:
    """å­¦è¡“çš„èƒŒæ™¯ã¨ç›®çš„ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    
    # Initialize LLM
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0.1,  # Low temperature for factual accuracy
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(AcademicBackground)
    
    # Get research question
    research_question = get_research_topic(state["messages"])
    
    # Format prompt
    formatted_prompt = ACADEMIC_BACKGROUND_PROMPT.format(research_question=research_question)
    
    # Generate background and objective
    result = structured_llm.invoke(formatted_prompt)
    
    print(f"DEBUG - Academic background generated: {result.background[:100]}...")
    
    return {
        "background": result.background,
        "objective": result.objective,
        "research_framework": result.research_framework
    }


def academic_framework_planner(state: OverallState, config: RunnableConfig) -> AcademicFrameworkState:
    """å­¦è¡“è«–æ–‡ã®å…¨ä½“ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    
    # Initialize LLM
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0.2,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    
    # Format background and objective
    background_data = state.get("academic_background", {})
    background_and_objective = f"""
## èƒŒæ™¯
{background_data.get('background', '')}

## ç›®çš„
{background_data.get('objective', '')}

## ç ”ç©¶ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
{background_data.get('research_framework', '')}
    """
    
    # Format prompt
    formatted_prompt = ACADEMIC_FRAMEWORK_PROMPT.format(
        background_and_objective=background_and_objective
    )
    
    # Generate framework
    result = llm.invoke(formatted_prompt)
    
    print(f"DEBUG - Academic framework generated: {len(result.content)} characters")
    
    # Parse the markdown response into sections (simplified parsing)
    content = result.content
    sections = {
        "introduction": "",
        "objective": "",
        "methods": "",
        "results": "",
        "discussion": "",
        "conclusion": ""
    }
    
    # Simple section extraction (this could be improved with better parsing)
    current_section = None
    lines = content.split('\n')
    
    for line in lines:
        line_lower = line.lower()
        if 'èƒŒæ™¯' in line or 'introduction' in line_lower:
            current_section = 'introduction'
        elif 'ç›®çš„' in line or 'objective' in line_lower:
            current_section = 'objective'
        elif 'æ–¹æ³•' in line or 'methods' in line_lower:
            current_section = 'methods'
        elif 'çµæžœ' in line or 'results' in line_lower:
            current_section = 'results'
        elif 'è€ƒå¯Ÿ' in line or 'discussion' in line_lower:
            current_section = 'discussion'
        elif 'çµè«–' in line or 'conclusion' in line_lower:
            current_section = 'conclusion'
        elif current_section and line.strip():
            sections[current_section] += line + '\n'
    
    return sections


def academic_abstract_generator(state: OverallState, config: RunnableConfig) -> AcademicAbstractState:
    """å­¦è¡“è«–æ–‡ã®ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    
    # Initialize LLM
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0.1,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(AcademicAbstract)
    
    # Create full paper draft from framework
    framework = state.get("academic_framework", {})
    full_paper_draft = f"""
# å­¦è¡“è«–æ–‡ãƒ‰ãƒ©ãƒ•ãƒˆ

## èƒŒæ™¯
{framework.get('introduction', '')}

## ç›®çš„
{framework.get('objective', '')}

## ææ–™ã¨æ–¹æ³•
{framework.get('methods', '')}

## çµæžœ
{framework.get('results', '')}

## è€ƒå¯Ÿ
{framework.get('discussion', '')}

## çµè«–
{framework.get('conclusion', '')}
    """
    
    # Format prompt
    formatted_prompt = ABSTRACT_GENERATOR_PROMPT.format(full_paper_draft=full_paper_draft)
    
    # Generate abstract
    result = structured_llm.invoke(formatted_prompt)
    
    print(f"DEBUG - Academic abstract generated: {len(result.abstract_text)} characters")
    
    return {
        "abstract_text": result.abstract_text,
        "key_findings": result.key_findings
    }


def literature_researcher(state: OverallState, config: RunnableConfig) -> LiteratureResearchState:
    """å…ˆè¡Œç ”ç©¶ãƒ»æ–‡çŒ®èª¿æŸ»ã‚’å®Ÿæ–½ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    
    # Initialize LLM
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0.1,  # Very low temperature for factual research
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(LiteratureResearch)
    
    # Get abstract for research
    abstract_data = state.get("academic_abstract", "")
    
    # Perform web search for literature
    research_question = get_research_topic(state["messages"])
    
    # Create focused search queries for academic sources
    academic_queries = [
        f"{research_question} å­¦è¡“ç ”ç©¶ è«–æ–‡",
        f"{research_question} å…¬å¼ãƒ‡ãƒ¼ã‚¿ çµ±è¨ˆ",
        f"{research_question} æ”¿åºœç™ºè¡¨ å…¬å¼æƒ…å ±"
    ]
    
    search_results = []
    for i, query in enumerate(academic_queries):
        try:
            # Use existing Google Search API functionality
            response = genai_client.models.generate_content(
                model=reasoning_model,
                contents=f"ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€ä¿¡é ¼æ€§ã®é«˜ã„å­¦è¡“çš„æƒ…å ±æºã‹ã‚‰äº‹å®Ÿæƒ…å ±ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„: {query}",
                config={
                    "tools": [{"google_search": {}}],
                    "temperature": 0.1,
                },
            )
            
            # Process search results safely
            if response and response.text:
                # Resolve URLs and get citations like in web_research
                if (hasattr(response, 'candidates') and 
                    response.candidates and 
                    len(response.candidates) > 0):
                    candidate = response.candidates[0]
                    if (hasattr(candidate, 'grounding_metadata') and 
                        candidate.grounding_metadata and 
                        hasattr(candidate.grounding_metadata, 'grounding_chunks') and
                        candidate.grounding_metadata.grounding_chunks is not None):
                        try:
                            resolved_urls = resolve_urls(candidate.grounding_metadata.grounding_chunks, i)
                            citations = get_citations(response, resolved_urls) if resolved_urls else None
                            if citations:
                                modified_text = insert_citation_markers(response.text, citations)
                            else:
                                modified_text = response.text
                            search_results.append(f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\nçµæžœ: {modified_text}\n---")
                        except Exception as citation_error:
                            print(f"Citation processing failed for '{query}': {citation_error}")
                            search_results.append(f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\nçµæžœ: {response.text}\n---")
                    else:
                        search_results.append(f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\nçµæžœ: {response.text}\n---")
                else:
                    search_results.append(f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\nçµæžœ: {response.text}\n---")
        except Exception as e:
            print(f"Literature search failed for '{query}': {e}")
            search_results.append(f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\nã‚¨ãƒ©ãƒ¼: æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ\n---")
    
    combined_search_results = "\n".join(search_results)
    
    # Format prompt with search results
    formatted_prompt = LITERATURE_RESEARCH_PROMPT.format(
        research_abstract=abstract_data
    ) + f"\n\n### æ¤œç´¢çµæžœ:\n{combined_search_results}"
    
    # Generate literature research
    result = structured_llm.invoke(formatted_prompt)
    
    print(f"DEBUG - Literature research completed: {len(result.factual_findings)} facts found")
    
    return {
        "factual_findings": result.factual_findings,
        "official_data": result.official_data,
        "reliable_sources": result.reliable_sources,
        "source_urls": result.source_urls
    }


def academic_synthesizer(state: OverallState, config: RunnableConfig):
    """æœ€çµ‚çš„ãªå­¦è¡“è«–æ–‡ã‚’çµ±åˆãƒ»ä½œæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    
    # Initialize LLM
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0.2,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    
    # Prepare data
    abstract_data = state.get("academic_abstract", "")
    literature_data = state.get("literature_research", {})
    
    # Format literature research
    literature_summary = f"""
## å…ˆè¡Œç ”ç©¶èª¿æŸ»çµæžœ

### ä¸»è¦ãªäº‹å®Ÿæƒ…å ±
{chr(10).join(f"- {fact}" for fact in literature_data.get('factual_findings', []))}

### é–¢é€£ã™ã‚‹å…¬å¼ãƒ‡ãƒ¼ã‚¿
{chr(10).join(f"- {data}" for data in literature_data.get('official_data', []))}

### ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æº
{chr(10).join(f"- {source}" for source in literature_data.get('reliable_sources', []))}

### å‚è€ƒURL
{chr(10).join(f"- {url}" for url in literature_data.get('source_urls', []))}
    """
    
    # Format prompt
    formatted_prompt = ACADEMIC_REPORT_SYNTHESIS_PROMPT.format(
        research_abstract=abstract_data,
        literature_research=literature_summary
    )
    
    # Generate final academic report
    result = llm.invoke(formatted_prompt)
    
    print(f"DEBUG - Academic report synthesized: {len(result.content)} characters")
    
    return {
        "academic_draft": result.content,
        "current_phase": "academic_synthesis_completed"
    }


def academic_reviewer(state: OverallState, config: RunnableConfig) -> AcademicReviewState:
    """å­¦è¡“è«–æ–‡ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿæ–½ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.reflection_model
    
    # Initialize LLM
    llm = ChatGoogleGenerativeAI(
        model=reasoning_model,
        temperature=0.3,  # Slightly higher temperature for critical analysis
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(AcademicReview)
    
    # Get academic draft for review
    academic_draft = state.get("academic_draft", "")
    
    # Format prompt
    formatted_prompt = ACADEMIC_REVIEW_PROMPT.format(academic_draft=academic_draft)
    
    # Generate review
    result = structured_llm.invoke(formatted_prompt)
    
    print(f"DEBUG - Academic review completed: revision_needed={result.revision_needed}")
    
    return {
        "overall_assessment": result.overall_assessment,
        "specific_improvements": result.specific_improvements,
        "speculation_issues": result.speculation_issues,
        "revision_needed": result.revision_needed,
        "revision_instructions": result.revision_instructions
    }


# ============================================================================
# ACADEMIC RESEARCH FRAMEWORK GRAPH (å­¦è¡“è«–æ–‡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç”¨ã‚°ãƒ©ãƒ•)
# ============================================================================

def build_academic_research_graph():
    """å­¦è¡“è«–æ–‡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ãæ–°ã—ã„ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰"""
    from langgraph.graph import StateGraph, START, END
    
    # Initialize academic research graph
    academic_builder = StateGraph(OverallState)
    
    # Add academic research nodes
    academic_builder.add_node("academic_background_generator", academic_background_generator)
    academic_builder.add_node("academic_framework_planner", academic_framework_planner) 
    academic_builder.add_node("academic_abstract_generator", academic_abstract_generator)
    academic_builder.add_node("literature_researcher", literature_researcher)
    academic_builder.add_node("academic_synthesizer", academic_synthesizer)
    academic_builder.add_node("academic_reviewer", academic_reviewer)
    
    # Build academic research flow
    academic_builder.add_edge(START, "academic_background_generator")
    academic_builder.add_edge("academic_background_generator", "academic_framework_planner")
    academic_builder.add_edge("academic_framework_planner", "academic_abstract_generator")
    academic_builder.add_edge("academic_abstract_generator", "literature_researcher")
    academic_builder.add_edge("literature_researcher", "academic_synthesizer")
    academic_builder.add_edge("academic_synthesizer", "academic_reviewer")
    academic_builder.add_edge("academic_reviewer", END)
    
    return academic_builder.compile(name="academic-research-agent")


# Build academic research graph
academic_graph = build_academic_research_graph()

# Export enhanced graph as primary (for general research queries)
# Use enhanced_graph for casual research, academic_graph for academic papers
graph = enhanced_graph